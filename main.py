import numpy as np
from transformers import DebertaTokenizer, DebertaForSequenceClassification
from gcn.train import *
import torch
from deberta.train import *

# def initialize_labels(num_samples, num_classes):
#     array = np.zeros((num_samples, num_classes), dtype=int)
#     random_indices = np.random.randint(0, num_classes, size=(num_samples,))
#     array[np.arange(num_samples), random_indices] = 1
#     return array

def test_model_on_entire_dataset(model_info, labels, test_mask):
    sess = model_info['session']
    placeholders = model_info['placeholders']
    model = model_info['model']
    support = model_info['support']
    features = model_info['features']

    feed_dict = construct_feed_dict(features, support, labels, test_mask, placeholders)

    predicted_probabilities = sess.run(model.predict(), feed_dict=feed_dict)
    predicted_labels = np.argmax(predicted_probabilities, axis=1)
    num_correct = np.sum(predicted_labels[test_mask] == np.argmax(labels[test_mask], axis=1))
    total_nodes = np.sum(test_mask)

    accuracy = num_correct / total_nodes
    print("Accuracy on entire dataset: {:.2f} %".format(accuracy*100))

if __name__=="__main__":
    #dataset loading
    adjacency_matrix, features, train_mask, validation_mask, test_mask, correct_labels = load_data("citeseer")
    # features shape - (3327, 3703)
    # no of nodes - 3327

    #gcn parameters
    model_info = {'session': None, 'placeholders': None, 'model': None, 'support':None, 'features':None}

    #deberta initialization
    cache_directory = "/home/p20200470/shardul/GLEM_final/deberta"
    model_name = "microsoft/deberta-base"
    tokenizer = DebertaTokenizer.from_pretrained(model_name, cache_dir = cache_directory)
    debertaModel = DebertaForSequenceClassification.from_pretrained(model_name, num_labels = len(correct_labels[0]), 
                                                                    cache_dir = cache_directory)
    input_ids_tensor, attention_masks_tensor = preprocess_deberta(features, tokenizer)

    all_dataset = CustomDataset(input_ids_tensor, attention_masks_tensor, correct_labels)
    all_dataloader = DataLoader(all_dataset, batch_size=32, shuffle=True, num_workers=4)

    device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
    #device = torch.device("cpu")
    debertaModel.to(device)

    #pseudo_labels = initialize_labels(adjacency_matrix.shape[0], len(correct_labels[0]))
    pseudo_labels = predict(debertaModel, all_dataloader, device)

    epochs = 15
    method = "GNN-first"

    for epoch in range(epochs):
        print("Main epoch: ",epoch)
        if method=="LM-first": 
            print("E-step: ") #(fix GNN, use LM)
            # 1. take info for each node as input (input_ids_tensor, attention_masks_tensor)
            # 2. take labels generated by GNN as input 
            debertaModel, pseudo_labels = train_deberta(debertaModel, input_ids_tensor, attention_masks_tensor, train_mask, 
                                                        validation_mask, test_mask, pseudo_labels, correct_labels, device)
            # 3. output model and labels for all nodes

            print("M-step: ") #(fix LM, use GNN)
            # 1. take token embeddings for each node as input (features)
            # 2. take labels generated by LM as input
            model_info, pseudo_labels = train_gcn(model_info, adjacency_matrix, features, train_mask, 
                                                  validation_mask, test_mask, pseudo_labels, correct_labels)
            # 3. output model and labels for all nodes
        else:
            print("M-step: ") #(fix LM, use GNN)
            # 1. take token embeddings for each node as input
            # 2. take labels generated by LM as input
            #model_info, pseudo_labels = train_gcn(model_info, adjacency_matrix, features, train_mask, 
            #                                      validation_mask, test_mask, pseudo_labels, correct_labels)
            # 3. output model and labels for all nodes

            print("E-step: ") #(fix GNN, use LM)
            # 1. take info for each node as input (input_ids_tensor, attention_masks_tensor)
            # 2. take labels generated by GNN as input
            debertaModel, pseudo_labels = train_deberta(debertaModel, input_ids_tensor, attention_masks_tensor, train_mask, 
                                                        validation_mask, test_mask, pseudo_labels, correct_labels, device)
            # 3. output model and labels for all nodes
        print()
    
    #final testing
    test_model_on_entire_dataset(model_info, correct_labels, test_mask)